{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate imbalanced classification model with different metrics\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_classification(n_samples=10000,n_features=2,n_redundant=0,n_clusters_per_class=1,weights=[.99],flip_y=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split into train and test sets with same ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.5,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.41732934  0.55863665]\n",
      " [-1.4614058   1.35976584]\n",
      " [-0.58693624  0.9968201 ]\n",
      " ...\n",
      " [-1.59841118  1.17467489]\n",
      " [-1.41822929  1.05623893]\n",
      " [ 1.76410883 -2.46736992]]\n",
      "[0 0 0 ... 0 0 1]\n",
      "[[-0.71103517  0.78141823]\n",
      " [-0.88507578  1.09642749]\n",
      " [-0.8415209   0.80868124]\n",
      " ...\n",
      " [ 0.14057403  0.25558277]\n",
      " [-1.67388325  1.614675  ]\n",
      " [-1.17385146  0.98460554]]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "print(y_train)\n",
    "print(X_test)\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg=LogisticRegression(solver='liblinear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_pred=reg.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.9954\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy : \",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision :  0.9655172413793104\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision : \",precision_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall :  0.56\n"
     ]
    }
   ],
   "source": [
    "print(\"Recall : \",recall_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F Measure :  0.7088607594936709\n"
     ]
    }
   ],
   "source": [
    "print(\"F Measure : \",f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score :  0.7798989898989899\n"
     ]
    }
   ],
   "source": [
    "print(\"ROC AUC score : \",roc_auc_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FBeta score :  0.8433734939759036\n"
     ]
    }
   ],
   "source": [
    "print(\"FBeta score : \",fbeta_score(y_test,y_pred,beta=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing under sampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple approach to using standard ML algorithms on an imbalance datasetis to change the training dataset to have more balanced class distribution\n",
    "\n",
    "This can be achieved by deleting / selecting examples form majority class referred to as \"Under sampling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\gaurav\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: scikit-learn>=0.23 in c:\\users\\gaurav\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (from imbalanced-learn) (0.23.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\gaurav\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (from imbalanced-learn) (1.18.5)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\gaurav\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (from imbalanced-learn) (0.15.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\gaurav\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (from imbalanced-learn) (1.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\gaurav\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (from scikit-learn>=0.23->imbalanced-learn) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_classification(n_samples=10000,n_features=2,n_redundant=0,n_clusters_per_class=1,weights=[.99,0.01],flip_y=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter  Counter({0: 9900, 1: 100})\n"
     ]
    }
   ],
   "source": [
    "print(\"Counter \",Counter(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define undersampled strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomUnderSampler(sampling_strategy=0.5)\n"
     ]
    }
   ],
   "source": [
    "undersample = RandomUnderSampler(sampling_strategy=0.5)\n",
    "print(undersample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit and apply the transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_under,y_under = undersample.fit_resample(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter :  Counter({0: 200, 1: 100})\n"
     ]
    }
   ],
   "source": [
    "print(\"Counter : \",Counter(y_under))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Over sampling the minority class using SMOTE (Synthetic Minority Oversampling Technique) algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " the smote algorithm is popular approach for over samling the minority class, this technique can be used to reduce imbalanced or to make the class distribution even\n",
    "    \n",
    " The eg below demonstrate using the smote class provided by the imbalanced learn library on a synthetic dataset. \n",
    "\n",
    "The initial class distribution is 1:100 and minority class is over sampled to 1:2 distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_classification(n_samples=10000,n_features=2,n_redundant=0,n_clusters_per_class=1,weights=[.99,0.01],flip_y=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter  Counter({0: 9900, 1: 100})\n"
     ]
    }
   ],
   "source": [
    "print(\"Counter \",Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SMOTE(sampling_strategy=0.5)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversample = SMOTE(sampling_strategy=0.5)\n",
    "oversample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit and apply the transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_over,y_over = oversample.fit_resample(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter  Counter({0: 9900, 1: 4950})\n"
     ]
    }
   ],
   "source": [
    "print(\"Counter \",Counter(y_over))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Over sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomely duplicate examples in the minority class\n",
    "\n",
    "\n",
    "Random over sampling involves randomly selecting examples from the minority class with replacement and adding them to training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Under Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly delete examples in the minority class\n",
    "\n",
    "\n",
    "Random under sampling involves randomly selecting examples from the minority class and deleting them from training dataset\n",
    "\n",
    "In this the majority class instances are discarded at random until a more balanced distribution is reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine under sampling and over sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data undersampling will delete examples from the majority class, whereas data oversampling will add examples to the majority class. These two approaches can be combined and used on a single training dataset.\n",
    "\n",
    "Given that there are so many different data sampling techniques to choose from, it can be confusing as to which methods to combine. Thankfully, there are common combinations that have been shown to work well in practice; some examples include:\n",
    "\n",
    "\n",
    "    Random Undersampling with SMOTE oversampling.\n",
    "    Tomek Links Undersampling with SMOTE oversampling.\n",
    "    Edited Nearest Neighbors Undersampling with SMOTE oversampling.\n",
    "\n",
    "\n",
    "These combinations can be applied manually to a given training dataset by first applying one sampling algorithm, then another. Thankfully, the imbalanced-learn library provides implementations of common combined data sampling techniques.\n",
    "\n",
    "The example below demonstrates how to use the SMOTEENN that combines both SMOTE oversampling of the minority class and Edited Nearest Neighbors undersampling of the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter  Counter({0: 9900, 1: 100})\n"
     ]
    }
   ],
   "source": [
    "print(\"Counter \",Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = SMOTEENN(sampling_strategy=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_over,y_over = sample.fit_resample(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter  Counter({0: 9492, 1: 4528})\n"
     ]
    }
   ],
   "source": [
    "print(\"Counter \",Counter(y_over))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost sensitive algorithm for imbalanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine learning algorithms assume that all misclassification errors made by a model are equal. This is often not the case for imbalanced classification problems, where missing a positive or minority class case is worse than incorrectly classifying an example from the negative or majority class.\n",
    "\n",
    "Cost-sensitive learning is a subfield of machine learning that takes the costs of prediction errors (and potentially other costs) into account when training a machine learning model. Many machine learning algorithms can be updated to be cost-sensitive, where the model is penalized for misclassification errors from one class more than the other, such as the minority class.\n",
    "\n",
    "The scikit-learn library provides this capability for a range of algorithms via the class_weight attribute specified when defining the model. A weighting can be specified that is inversely proportional to the class distribution.\n",
    "\n",
    "If the class distribution was 0.99 to 0.01 for the majority and minority classes, then the class_weight argument could be defined as a dictionary that defines a penalty of 0.01 for errors made for the majority class and a penalty of 0.99 for errors made with the minority class, e.g. {0:0.01, 1:0.99}.\n",
    "\n",
    "This is a useful heuristic and can be configured automatically by setting the class_weight argument to the string ‘balanced‘.\n",
    "\n",
    "The example below demonstrates how to define and fit a cost-sensitive logistic regression model on an imbalanced classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_classification(n_samples=10000,n_features=2,n_redundant=0,n_clusters_per_class=1,weights=[.99],flip_y=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg2=LogisticRegression(solver='liblinear',class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced', solver='liblinear')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.5,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced', solver='liblinear')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F Measure :  0.6369426751592356\n"
     ]
    }
   ],
   "source": [
    "print(\"F Measure : \",f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
